{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mimrancomsats/ProgrammingforAI_FALL24/blob/main/Lab_Exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKBOgALplcmv"
   },
   "source": [
    "## **Quiz-2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5VxQWNuli1L"
   },
   "source": [
    "Perform following tasks on the provided Reviews Dataset.\n",
    "* Drop words if not alphabets.\n",
    "* Tokenize the sentence.\n",
    "* Perform lemitization.\n",
    "* Vectorize using bigram and trigram techniques.\n",
    "* Apply Random Forest algorithm with 150 trees.\n",
    "* Evaluate overall accuracy of the model and class-wise precision ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "stopwords.words('english')\n",
    "exclude = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./reviews_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China had role in Yukos split-up\\n \\n China le...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oil rebounds from weather effect\\n \\n Oil pric...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indonesia 'declines debt freeze'\\n \\n Indonesi...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$1m payoff for former Shell boss\\n \\n Shell is...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US bank in $515m SEC settlement\\n \\n Five Bank...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                news      type\n",
       "0  China had role in Yukos split-up\\n \\n China le...  business\n",
       "1  Oil rebounds from weather effect\\n \\n Oil pric...  business\n",
       "2  Indonesia 'declines debt freeze'\\n \\n Indonesi...  business\n",
       "3  $1m payoff for former Shell boss\\n \\n Shell is...  business\n",
       "4  US bank in $515m SEC settlement\\n \\n Five Bank...  business"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in 'type':\n",
      "['business' 'entertainment' 'politics' 'sport' 'tech']\n"
     ]
    }
   ],
   "source": [
    "unique_classes = df['type'].unique()\n",
    "print(\"Unique classes in 'type':\")\n",
    "print(unique_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   news      type\n",
      "0     [china, had, role, in, yukos, china, lent, rus...  business\n",
      "1     [oil, rebound, from, weather, effect, oil, pri...  business\n",
      "2     [indonesia, debt, indonesia, no, longer, need,...  business\n",
      "3     [payoff, for, former, shell, bos, shell, is, t...  business\n",
      "4     [u, bank, in, sec, settlement, five, bank, of,...  business\n",
      "...                                                 ...       ...\n",
      "2220  [microsoft, launch, it, own, search, microsoft...      tech\n",
      "2221  [warning, about, junk, mail, deluge, the, amou...      tech\n",
      "2222  [microsoft, get, the, blogging, bug, software,...      tech\n",
      "2223  [gamers, snap, up, new, sony, psp, gamers, hav...      tech\n",
      "2224  [apple, laptop, is, the, apple, powerbook, ha,...      tech\n",
      "\n",
      "[2225 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_non_alpha(tokens):\n",
    "    return [word for word in tokens if word.isalpha()]\n",
    "\n",
    "def lemmatize_words(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word.lower()) for word in tokens]\n",
    "\n",
    "\n",
    "df['news'] = df['news'].str.lower()\n",
    "\n",
    "df['news'] = df['news'].apply(tokenize_text)\n",
    "\n",
    "df['news'] = df['news'].apply(remove_non_alpha)\n",
    "\n",
    "df['news'] = df['news'].apply(lemmatize_words)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['news'] = df['news'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       china had role in yukos china lent russia to h...\n",
      "1       oil rebound from weather effect oil price reco...\n",
      "2       indonesia debt indonesia no longer need the de...\n",
      "3       payoff for former shell bos shell is to pay to...\n",
      "4       u bank in sec settlement five bank of america ...\n",
      "                              ...                        \n",
      "2220    microsoft launch it own search microsoft ha un...\n",
      "2221    warning about junk mail deluge the amount of s...\n",
      "2222    microsoft get the blogging bug software giant ...\n",
      "2223    gamers snap up new sony psp gamers have bought...\n",
      "2224    apple laptop is the apple powerbook ha been ch...\n",
      "Name: news, Length: 2225, dtype: object\n",
      "0       business\n",
      "1       business\n",
      "2       business\n",
      "3       business\n",
      "4       business\n",
      "          ...   \n",
      "2220        tech\n",
      "2221        tech\n",
      "2222        tech\n",
      "2223        tech\n",
      "2224        tech\n",
      "Name: type, Length: 2225, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df['news']\n",
    "Y = df['type']\n",
    "\n",
    "print(X)\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(Y)\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1780,)\n",
      "(445,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_n_gram: (1780, 252894)\n",
      "Shape of X_test_n_gram: (445, 252894)\n",
      "Accuracy: 0.8697\n",
      "Confusion Matrix:\n",
      "[[106   1   3   2   3]\n",
      " [ 10  51   0  10   1]\n",
      " [  4   1  65   6   0]\n",
      " [  0   0   0 102   0]\n",
      " [  9   2   0   6  63]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87       115\n",
      "           1       0.93      0.71      0.80        72\n",
      "           2       0.96      0.86      0.90        76\n",
      "           3       0.81      1.00      0.89       102\n",
      "           4       0.94      0.79      0.86        80\n",
      "\n",
      "    accuracy                           0.87       445\n",
      "   macro avg       0.89      0.85      0.87       445\n",
      "weighted avg       0.88      0.87      0.87       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "X_train_n_gram = cv.fit_transform(X_train)\n",
    "X_test_n_gram = cv.transform(X_test)\n",
    "\n",
    "print(f\"Shape of X_train_n_gram: {X_train_n_gram.shape}\")\n",
    "print(f\"Shape of X_test_n_gram: {X_test_n_gram.shape}\")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "\n",
    "rf.fit(X_train_n_gram, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test_n_gram)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_n_gram: (1780, 480193)\n",
      "Shape of X_test_n_gram: (445, 480193)\n",
      "Overall Accuracy: 0.7640\n",
      "Confusion Matrix:\n",
      "[[ 94   0   4  16   1]\n",
      " [  5  38   0  27   2]\n",
      " [  2   1  59  14   0]\n",
      " [  0   1   0 101   0]\n",
      " [  7   2   1  22  48]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84       115\n",
      "           1       0.90      0.53      0.67        72\n",
      "           2       0.92      0.78      0.84        76\n",
      "           3       0.56      0.99      0.72       102\n",
      "           4       0.94      0.60      0.73        80\n",
      "\n",
      "    accuracy                           0.76       445\n",
      "   macro avg       0.84      0.74      0.76       445\n",
      "weighted avg       0.83      0.76      0.77       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(3, 3))\n",
    "X_train_n_gram = cv.fit_transform(X_train)\n",
    "X_test_n_gram = cv.transform(X_test)\n",
    "\n",
    "print(f\"Shape of X_train_n_gram: {X_train_n_gram.shape}\")\n",
    "print(f\"Shape of X_test_n_gram: {X_test_n_gram.shape}\")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "rf.fit(X_train_n_gram, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test_n_gram)\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Bi-Gram and Tri-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_n_gram: (1780, 733087)\n",
      "Shape of X_test_n_gram: (445, 733087)\n",
      "Overall Accuracy: 0.8629\n",
      "Confusion Matrix:\n",
      "[[109   0   4   2   0]\n",
      " [ 12  46   1  13   0]\n",
      " [  4   0  65   7   0]\n",
      " [  0   0   0 102   0]\n",
      " [ 11   1   0   6  62]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.87       115\n",
      "           1       0.98      0.64      0.77        72\n",
      "           2       0.93      0.86      0.89        76\n",
      "           3       0.78      1.00      0.88       102\n",
      "           4       1.00      0.78      0.87        80\n",
      "\n",
      "    accuracy                           0.86       445\n",
      "   macro avg       0.90      0.84      0.86       445\n",
      "weighted avg       0.88      0.86      0.86       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 3))\n",
    "X_train_n_gram = cv.fit_transform(X_train)\n",
    "X_test_n_gram = cv.transform(X_test)\n",
    "\n",
    "print(f\"Shape of X_train_n_gram: {X_train_n_gram.shape}\")\n",
    "print(f\"Shape of X_test_n_gram: {X_test_n_gram.shape}\")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "rf.fit(X_train_n_gram, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test_n_gram)\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyykdIyFaBBcDVOB+/+hJp",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
